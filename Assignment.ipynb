{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "1) Meet Patel (C0871240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables for your Assignment:\n",
    "\n",
    "1) Describe the MODEL that you are using in your code as the Model for your Embedding. Research and Discuss WHY you choose that model. How is it of particular value to your Project Business Domain.\n",
    "\n",
    "2) Research and select a MODEL for your Embedding (and therefore later your Project), and support and defend your reasoning and decision making as to why you choose that MODEL for your Use Cases and Business Domain:\n",
    "\n",
    "3) If you were doing this at work: What licensing and pricing considerations for using the APIs would factor into account?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Model Description and Selection**: The model used in the code is the `jinaai/jina-embeddings-v2-base-en`¹. This is an English, monolingual embedding model that supports a sequence length of 8192¹. It is based on a Bert architecture (JinaBert) that supports the symmetric bidirectional variant of ALiBi to allow longer sequence length¹. The backbone `jina-bert-v2-base-en` is pretrained on the C4 dataset¹. The model is further trained on Jina AI's collection of more than 400 millions of sentence pairs and hard negatives¹. These pairs were obtained from various domains and were carefully selected through a thorough cleaning process¹. The model was chosen for its ability to handle long sequences, making it particularly useful for tasks that require processing long documents, including long document retrieval, semantic textual similarity, text reranking, recommendation, RAG and LLM-based generative search, etc¹.\n",
    "\n",
    "2) **Model Value for Business Domain**: The `jinaai/jina-embeddings-v2-base-en` model is of particular value to many business domains due to its extended context capabilities⁴. For instance, in the legal domain, it can capture and analyze intricate details in extensive legal texts effectively⁴. In the medical research domain, it can holistically embed scientific papers for advanced analytics and discoveries⁴. The model's ability to handle long sequences makes it especially useful when processing long documents is needed¹.\n",
    "\n",
    "3) **Licensing and Pricing Considerations**: The `jinaai/jina-embeddings-v2-base-en` model is freely available under the Apache 2.0 license³. This means it can be used without any cost, making it a cost-effective choice for businesses. However, if you plan to use the model in a commercial product, you should review the terms of the Apache 2.0 license to ensure compliance. As for API usage, pricing would depend on the specific API provider and usage requirements. It's important to consider factors such as the number of API calls needed, data transfer costs, and whether the API provider offers a free tier or volume discounts. Always review the API provider's pricing documentation for the most accurate information.\n",
    "\n",
    "Source: Conversation with Bing, 11/18/2023\n",
    "(1) jinaai/jina-embeddings-v2-base-en · Hugging Face. https://huggingface.co/jinaai/jina-embeddings-v2-base-en.\n",
    "(2) jina-embeddings-v2-base-en model | Clarifai - The World's AI. https://clarifai.com/jinaai/jina-embeddings/models/jina-embeddings-v2-base-en.\n",
    "(3) Jina AI's Open-Source Embedding Model Outperforms OpenAI's Ada - InfoQ. https://www.infoq.com/news/2023/11/jina-ai-embeddings/.\n",
    "(4) jinaai/jina-embeddings-v2-small-en · Hugging Face. https://huggingface.co/jinaai/jina-embeddings-v2-small-en.\n",
    "(5) Embedding API - jinaai.cn. https://www.jinaai.cn/embeddings/.\n",
    "(6) Jina AI’s jina-embeddings-v2: an open source text embedding model that .... https://www.baseten.co/blog/jina-embeddings-v2-open-source-text-embedding-that-matches-openai-ada-002/.\n",
    "(7) Jina Embeddings - Finetuner documentation. https://finetuner.jina.ai/get-started/pretrained/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before using Pretrained model from Hugging Face, Lets create our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\n",
    "    'I love machine learning',\n",
    "    'I love deep learning',\n",
    "    'Deep learning is a subfield of machine learning',\n",
    "    'AI is fascinating',\n",
    "    'Machine learning is fascinating'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Explanation\n",
    "\n",
    "1. **Embedding Layer**: The first layer is an Embedding layer, which is used for word embeddings. Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. The Embedding layer takes the integer-encoded vocabulary (`total_words`) and the length of input sequences (`max_sequence_len-1`) as inputs and produces dense vectors of fixed size (10 in this case). This layer can only be used as the first layer in a model.\n",
    "\n",
    "2. **LSTM Layer**: The next layer is an LSTM (Long Short-Term Memory) layer with 50 units. LSTM is a type of recurrent neural network (RNN) that can learn and remember over long sequences and is not prone to the vanishing gradient problem, which is a common issue with traditional RNNs. This makes LSTMs useful for processing and making predictions based on time series data or any data where the temporal dynamics are important.\n",
    "\n",
    "3. **Dense Layer**: The final layer is a Dense layer, which is a regular densely-connected neural network layer. It implements the operation: `output = activation(dot(input, kernel) + bias)`. Here, `total_words` is the dimensionality of the output space and `softmax` is the activation function. The softmax function outputs a vector that represents the probability distribution of a list of potential outcomes.\n",
    "\n",
    "4. **Compilation**: Finally, the model is compiled with the `adam` optimizer and the `categorical_crossentropy` loss function, which is suitable for multi-class classification problems. The model's performance is measured with the `accuracy` metric during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 7, 10)             120       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50)                12200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                612       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12932 (50.52 KB)\n",
      "Trainable params: 12932 (50.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))  # Embedding layer\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.4852 - accuracy: 0.0556\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4815 - accuracy: 0.3889\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4778 - accuracy: 0.2778\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4741 - accuracy: 0.2778\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4702 - accuracy: 0.2778\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4663 - accuracy: 0.2778\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4622 - accuracy: 0.2778\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4578 - accuracy: 0.2778\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4532 - accuracy: 0.2778\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4483 - accuracy: 0.2778\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4431 - accuracy: 0.2778\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4374 - accuracy: 0.2778\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4313 - accuracy: 0.2778\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4247 - accuracy: 0.2778\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4174 - accuracy: 0.2778\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4096 - accuracy: 0.2778\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4010 - accuracy: 0.2778\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3917 - accuracy: 0.2778\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3815 - accuracy: 0.2778\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3703 - accuracy: 0.2778\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3581 - accuracy: 0.2778\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.3448 - accuracy: 0.2778\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3303 - accuracy: 0.2778\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3146 - accuracy: 0.2778\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2977 - accuracy: 0.2778\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2798 - accuracy: 0.2778\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2609 - accuracy: 0.2778\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2415 - accuracy: 0.2778\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2221 - accuracy: 0.2778\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2038 - accuracy: 0.2778\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1877 - accuracy: 0.2778\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1755 - accuracy: 0.2778\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1685 - accuracy: 0.2778\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1663 - accuracy: 0.2778\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1662 - accuracy: 0.2778\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1644 - accuracy: 0.2778\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1586 - accuracy: 0.2778\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1491 - accuracy: 0.2778\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1375 - accuracy: 0.2778\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1258 - accuracy: 0.2778\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1152 - accuracy: 0.2778\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1065 - accuracy: 0.2778\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0996 - accuracy: 0.2778\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0941 - accuracy: 0.2778\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0896 - accuracy: 0.2778\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0855 - accuracy: 0.2778\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0813 - accuracy: 0.2778\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0770 - accuracy: 0.2778\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0723 - accuracy: 0.2778\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0672 - accuracy: 0.2778\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0620 - accuracy: 0.2778\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0567 - accuracy: 0.2778\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0515 - accuracy: 0.2778\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0465 - accuracy: 0.2778\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0419 - accuracy: 0.2778\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0377 - accuracy: 0.2778\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0338 - accuracy: 0.2778\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0300 - accuracy: 0.2778\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0262 - accuracy: 0.2778\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0223 - accuracy: 0.2778\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0183 - accuracy: 0.2778\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0142 - accuracy: 0.2778\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0100 - accuracy: 0.2778\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0059 - accuracy: 0.2778\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0018 - accuracy: 0.2778\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9979 - accuracy: 0.2778\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9941 - accuracy: 0.2778\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9904 - accuracy: 0.2778\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9867 - accuracy: 0.2778\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9829 - accuracy: 0.2778\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9789 - accuracy: 0.2778\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9748 - accuracy: 0.2778\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9705 - accuracy: 0.2778\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9661 - accuracy: 0.2778\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9616 - accuracy: 0.2778\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9570 - accuracy: 0.2778\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9525 - accuracy: 0.2778\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9478 - accuracy: 0.2778\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9430 - accuracy: 0.2778\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9380 - accuracy: 0.2778\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9327 - accuracy: 0.2778\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9272 - accuracy: 0.2778\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9214 - accuracy: 0.2778\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9152 - accuracy: 0.2778\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9088 - accuracy: 0.2778\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9021 - accuracy: 0.2778\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8950 - accuracy: 0.2778\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8876 - accuracy: 0.2778\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8797 - accuracy: 0.2778\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8713 - accuracy: 0.2778\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8624 - accuracy: 0.2778\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8531 - accuracy: 0.2778\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8432 - accuracy: 0.2778\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8328 - accuracy: 0.2778\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8218 - accuracy: 0.2778\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8100 - accuracy: 0.2778\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7976 - accuracy: 0.3333\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7845 - accuracy: 0.3333\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7706 - accuracy: 0.3333\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.7559 - accuracy: 0.3889\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.7403 - accuracy: 0.4444\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7238 - accuracy: 0.4444\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7065 - accuracy: 0.4444\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6883 - accuracy: 0.5000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6693 - accuracy: 0.5000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6495 - accuracy: 0.5000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6290 - accuracy: 0.5000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6079 - accuracy: 0.5000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5862 - accuracy: 0.5556\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5642 - accuracy: 0.5556\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5420 - accuracy: 0.5556\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5197 - accuracy: 0.5556\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4975 - accuracy: 0.6111\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4754 - accuracy: 0.5556\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4536 - accuracy: 0.5556\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4321 - accuracy: 0.5556\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4110 - accuracy: 0.5556\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3901 - accuracy: 0.5556\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3696 - accuracy: 0.6667\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3493 - accuracy: 0.6667\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3292 - accuracy: 0.6667\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3091 - accuracy: 0.6667\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2891 - accuracy: 0.6667\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2693 - accuracy: 0.6667\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2495 - accuracy: 0.6667\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2298 - accuracy: 0.6667\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2103 - accuracy: 0.6667\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1910 - accuracy: 0.7222\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1720 - accuracy: 0.7222\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1534 - accuracy: 0.7222\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1350 - accuracy: 0.7778\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1168 - accuracy: 0.7222\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0985 - accuracy: 0.7778\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0807 - accuracy: 0.7778\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0636 - accuracy: 0.7222\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0469 - accuracy: 0.7778\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0303 - accuracy: 0.7222\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0138 - accuracy: 0.7778\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9980 - accuracy: 0.7778\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9828 - accuracy: 0.7222\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9680 - accuracy: 0.7778\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9534 - accuracy: 0.7222\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9392 - accuracy: 0.7778\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9255 - accuracy: 0.7778\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9122 - accuracy: 0.7222\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8993 - accuracy: 0.7778\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8865 - accuracy: 0.7778\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8740 - accuracy: 0.7778\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8620 - accuracy: 0.7778\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8504 - accuracy: 0.7778\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8390 - accuracy: 0.7778\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8278 - accuracy: 0.7778\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8170 - accuracy: 0.7778\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8066 - accuracy: 0.7778\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7964 - accuracy: 0.7778\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7865 - accuracy: 0.7778\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7768 - accuracy: 0.7778\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7675 - accuracy: 0.7778\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7584 - accuracy: 0.7778\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7495 - accuracy: 0.7778\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7408 - accuracy: 0.7778\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7323 - accuracy: 0.7778\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7240 - accuracy: 0.7778\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7159 - accuracy: 0.7778\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7080 - accuracy: 0.7778\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7002 - accuracy: 0.7778\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6926 - accuracy: 0.7778\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6851 - accuracy: 0.7778\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6778 - accuracy: 0.7778\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6705 - accuracy: 0.7778\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6634 - accuracy: 0.7778\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6564 - accuracy: 0.7778\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6494 - accuracy: 0.7778\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6426 - accuracy: 0.7778\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6358 - accuracy: 0.7778\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6291 - accuracy: 0.7778\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6224 - accuracy: 0.7778\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6158 - accuracy: 0.7778\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6093 - accuracy: 0.7778\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6027 - accuracy: 0.7778\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5963 - accuracy: 0.7778\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5898 - accuracy: 0.7778\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5833 - accuracy: 0.7778\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5769 - accuracy: 0.7778\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5705 - accuracy: 0.7778\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5641 - accuracy: 0.7778\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5577 - accuracy: 0.7778\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5513 - accuracy: 0.7778\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5449 - accuracy: 0.8889\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5385 - accuracy: 0.8889\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5322 - accuracy: 0.8889\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5258 - accuracy: 0.8889\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5194 - accuracy: 0.8889\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5130 - accuracy: 0.8889\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5067 - accuracy: 0.8889\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5003 - accuracy: 0.8889\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4940 - accuracy: 0.8889\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4877 - accuracy: 0.8889\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4813 - accuracy: 0.8889\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4751 - accuracy: 0.8889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eccd89d110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Splitting data into predictors and label\n",
    "X = input_sequences[:,:-1]\n",
    "y = input_sequences[:,-1]\n",
    "\n",
    "# One-hot encoding the labels\n",
    "y = to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Training the model\n",
    "model.fit(X, y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for extracting the embeddings from our trained embedding layer we are using the code as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Create a dictionary to store the embeddings\n",
    "word_embeddings = {}\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    word_embeddings[word] = weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1577532  -0.26447624 -0.27063128 -0.20841956  0.2808393  -0.28767025\n",
      " -0.3072741   0.08063859  0.25579762  0.16986361]\n"
     ]
    }
   ],
   "source": [
    "print(word_embeddings['machine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Now use a model from Hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation: The transformers library, which provides pre-trained models for various text-related tasks, is installed using the command !pip install transformers.\n",
    "\n",
    "Imports: The AutoModel class from the transformers library and the norm function from the numpy.linalg module are imported.\n",
    "\n",
    "Cosine Similarity Function: A function named cos_sim is defined to calculate the cosine similarity between two vectors. This measure is used to determine the cosine of the angle between two non-zero vectors, providing a measure of their similarity.\n",
    "\n",
    "Model Loading: A pre-trained model, ‘jinaai/jina-embeddings-v2-base-en’, is loaded using the AutoModel.from_pretrained method. The trust_remote_code=True argument is required to use the encode method of the model.\n",
    "\n",
    "Encoding and Similarity Calculation: Two sentences, ‘How is the weather today?’ and ‘What is the current weather like today?’, are encoded using the pre-trained model. The cosine similarity between the resulting embeddings is then calculated using the cos_sim function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.33.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gurda\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\gurda\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "C:\\Users\\gurda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading config.json: 100%|██████████| 1.18k/1.18k [00:00<00:00, 1.16MB/s]\n",
      "C:\\Users\\gurda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gurda\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading configuration_bert.py: 100%|██████████| 8.24k/8.24k [00:00<?, ?B/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-implementation:\n",
      "- configuration_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading modeling_bert.py: 100%|██████████| 97.5k/97.5k [00:00<00:00, 5.89MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/jina-bert-implementation:\n",
      "- modeling_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading model.safetensors: 100%|██████████| 275M/275M [00:06<00:00, 41.7MB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 373/373 [00:00<00:00, 373kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 5.33MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 712k/712k [00:00<00:00, 8.37MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9341315\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "from transformers import AutoModel\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True) # trust_remote_code is needed to use the encode method\n",
    "embeddings = model.encode(['How is the weather today?', 'What is the current weather like today?'])\n",
    "# print(cos_sim(embeddings[0], embeddings[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34827104, -0.60091805,  0.6022362 , ..., -0.2523272 ,\n",
       "         0.23249894, -0.7026478 ],\n",
       "       [-0.11724894, -0.89896137,  0.4500913 , ..., -0.02847653,\n",
       "        -0.22871459, -0.42282885]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
